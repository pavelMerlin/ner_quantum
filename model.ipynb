{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "# !pip install transformers datasets evaluate seqeval\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "from transformers import create_optimizer\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "from transformers import pipeline\n",
    "from tensorflow.keras.models import load_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T19:45:04.671351200Z",
     "start_time": "2023-11-23T19:45:04.514853800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "# Define the file paths for the training and evaluation\n",
    "PATH_TO_TRAIN_DATA = \"../tokenaized_data/train_data.json\"\n",
    "PATH_TO_EVAL_DATA = \"../tokenaized_data/eval_data.json\"\n",
    "\n",
    "# Load the dataset from JSON files using the datasets library\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": PATH_TO_TRAIN_DATA,\n",
    "        \"evaluation\": PATH_TO_EVAL_DATA,\n",
    "    },\n",
    "    field=\"data\"  # Indicate the field in the JSON files where the actual data is located\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T19:45:10.593958Z",
     "start_time": "2023-11-23T19:45:04.734002500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "# Import the AutoTokenizer class from the transformers library\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Create an instance of AutoTokenizer and load the tokenizer for the \"distilbert-base-uncased\" model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T19:45:12.724134900Z",
     "start_time": "2023-11-23T19:45:10.593958Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/642 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23d1be3803ba4bdba33a6102c8e64345"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/161 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ffc0093659a40609ddffd1d5363184e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ner_tags', 'id', 'tokens', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 642\n",
      "    })\n",
      "    evaluation: Dataset({\n",
      "        features: ['ner_tags', 'id', 'tokens', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 161\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# # Retrieves the first example from the training split of the dataset\n",
    "# example = dataset[\"train\"][0]\n",
    "#\n",
    "# # Tokenizes the input text using the previously loaded tokenizer.\n",
    "# # is_split_into_words=True indicates that the input is already split into words.\n",
    "# tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "#\n",
    "# # Converts the token IDs back to tokens (words or subwords)\n",
    "# tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "\n",
    "# Define a function to tokenize inputs and align labels for NER\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes the inputs using the tokenizer, specifying truncation\n",
    "    and that the input is already split into words.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): Dictionary containing data for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        dict: Tokenized data with aligned labels.\n",
    "    \"\"\"\n",
    "    # Tokenizes the inputs using the tokenizer, specifying truncation and that the input is already split into words\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    # Initialize an empty list to store aligned labels\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through examples and their NER tags\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        # Map tokens to their respective word\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        # Align labels with tokenized inputs\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Set special tokens to -100\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)  # Append the label IDs for this example\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels  # Add aligned label IDs to tokenized inputs\n",
    "    return tokenized_inputs  # Return the tokenized inputs with labels aligned\n",
    "\n",
    "# Apply the tokenize_and_align_labels function to the entire dataset in a batched manner\n",
    "# This processes the dataset for NER by tokenizing inputs and aligning labels\n",
    "tokenized_data = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "print(tokenized_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T19:45:20.197091100Z",
     "start_time": "2023-11-23T19:45:12.755418500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "# Create an instance of DataCollatorForTokenClassification\n",
    "# This class prepares batches of tokenized inputs and labels for token classification\n",
    "# The tokenizer argument specifies the tokenizer to use for tokenization\n",
    "# return_tensors=\"tf\" configures the output to be TensorFlow tensors\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T19:45:22.301068400Z",
     "start_time": "2023-11-23T19:45:20.293251800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "# Conduct sequence labeling evaluation\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T19:45:28.763385200Z",
     "start_time": "2023-11-23T19:45:22.281646Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "# List of labels used in Named Entity Recognition (NER)\n",
    "label_list = [\"0\", \"B-geo\", \"I-geo\"]\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Computes evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        p (tuple): A tuple containing predictions and true labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing precision, recall, F1-score, and accuracy metrics.\n",
    "    \"\"\"\n",
    "    # Extract predictions and true labels from the input tuple\n",
    "    predictions, labels = p\n",
    "\n",
    "    # Convert predicted labels to indices with maximum probability along the last axis\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Convert predicted and true labels to label strings from label_list, excluding padding tokens (-100)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Define a checkpoint path for saving model weights\n",
    "    checkpoint_path = \"runs/cp-{epoch:04d}.ckpt\"\n",
    "\n",
    "    # Save model weights at epoch 0\n",
    "    model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "    # Compute NER evaluation metrics using seqeval library\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # Return a dictionary containing precision, recall, F1-score, and accuracy metrics\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T19:45:29.013387Z",
     "start_time": "2023-11-23T19:45:28.841575500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "# Maps numerical IDs to their respective labels. Provides the reverse mapping, associating labels with their corresponding numerical id.\n",
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-geo\",\n",
    "    2: \"I-geo\",\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-geo\": 1,\n",
    "    \"I-geo\": 2,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T19:45:29.028669300Z",
     "start_time": "2023-11-23T19:45:29.013387Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForTokenClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained model (\"distilbert-base-uncased\") for token classification\n",
    "# Set the number of labels to 3 (assuming 3 labels in this case: \"O\", \"B-geo\", \"I-geo\")\n",
    "# Provide mappings between numerical id and labels using id2label and label2id dictionaries\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T19:45:34.780149300Z",
     "start_time": "2023-11-23T19:45:29.075524900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# Prepare a TensorFlow dataset for training\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_data[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "# Prepare a TensorFlow dataset for validation\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_data[\"evaluation\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T19:45:41.228395700Z",
     "start_time": "2023-11-23T19:45:34.780149300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "# Number of samples processed in each training batch\n",
    "batch_size = 4\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 10\n",
    "\n",
    "# Calculate the total number of training steps\n",
    "num_train_steps = (len(tokenized_data[\"train\"]) // batch_size) * num_train_epochs\n",
    "\n",
    "# Create optimizer and learning rate scheduler\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=2e-5,  # Initial learning rate\n",
    "    num_train_steps=num_train_steps,  # Total number of training steps\n",
    "    weight_decay_rate=0.01,  # Rate of weight decay for regularization\n",
    "    num_warmup_steps=0,  # Number of warm-up steps for learning rate warm-up\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T19:45:41.369125100Z",
     "start_time": "2023-11-23T19:45:41.228395700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer=optimizer, metrics=['accuracy'])  # No loss argument!\n",
    "# Compiles the model\n",
    "model.compile(optimizer=optimizer)  # No loss argument!\n",
    "#\n",
    "# model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), loss=tf.keras.losses.Loss(),\n",
    "#               metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
    "#                        tf.keras.metrics.FalseNegatives()])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T19:45:41.656152400Z",
     "start_time": "2023-11-23T19:45:41.369125100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "# Create a KerasMetricCallback for tracking custom evaluation metrics during training\n",
    "# Uses the compute_metrics function for evaluation on the validation dataset\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T19:45:41.687274Z",
     "start_time": "2023-11-23T19:45:41.656152400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "# Create a list of callbacks containing the KerasMetricCallback\n",
    "callbacks = [metric_callback]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T19:45:42.272444800Z",
     "start_time": "2023-11-23T19:45:41.702682600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40/40 [==============================] - 204s 3s/step - loss: 0.3076 - val_loss: 0.1197 - precision: 0.5307 - recall: 0.4231 - f1: 0.4708 - accuracy: 0.9473\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spark\\PycharmProjects\\ner_quantum\\venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: 0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 90s 2s/step - loss: 0.0711 - val_loss: 0.0483 - precision: 0.8025 - recall: 0.8951 - f1: 0.8463 - accuracy: 0.9870\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 90s 2s/step - loss: 0.0305 - val_loss: 0.0329 - precision: 0.8571 - recall: 0.9021 - f1: 0.8790 - accuracy: 0.9892\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 90s 2s/step - loss: 0.0192 - val_loss: 0.0298 - precision: 0.8515 - recall: 0.9021 - f1: 0.8761 - accuracy: 0.9889\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 90s 2s/step - loss: 0.0135 - val_loss: 0.0245 - precision: 0.8833 - recall: 0.9266 - f1: 0.9044 - accuracy: 0.9922\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 90s 2s/step - loss: 0.0077 - val_loss: 0.0303 - precision: 0.8650 - recall: 0.9406 - f1: 0.9012 - accuracy: 0.9903\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 90s 2s/step - loss: 0.0053 - val_loss: 0.0221 - precision: 0.9076 - recall: 0.9615 - f1: 0.9338 - accuracy: 0.9944\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 89s 2s/step - loss: 0.0044 - val_loss: 0.0216 - precision: 0.8878 - recall: 0.9406 - f1: 0.9134 - accuracy: 0.9933\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 90s 2s/step - loss: 0.0031 - val_loss: 0.0262 - precision: 0.8893 - recall: 0.9545 - f1: 0.9207 - accuracy: 0.9933\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 91s 2s/step - loss: 0.0040 - val_loss: 0.0252 - precision: 0.9223 - recall: 0.9126 - f1: 0.9174 - accuracy: 0.9926\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x26f01d43700>"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using the prepared TensorFlow datasets for training and validation\n",
    "# The x parameter refers to the training dataset, `validation_data` to the validation dataset\n",
    "# epochs specifies the number of training epochs\n",
    "# callbacks contains the list of callbacks\n",
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=num_train_epochs, callbacks=callbacks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T20:02:40.127508500Z",
     "start_time": "2023-11-23T19:45:42.272444800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The path to the folder with model\n",
    "model_path = \"../runs\"\n",
    "\n",
    "# Saving the model to the \"runs\" directory\n",
    "model.save(\"runs\")\n",
    "# model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "text = \"In the heart of every mountain range lies a story as ancient as time itself, inscribed within the very fabric of stone and ice. These narratives weave tales of human endeavor, courage, and the relentless pursuit of conquering the unconquerable. From the serene and awe-inspiring Sierra Nevada to the rugged expanse of the Swiss Alps these colossal peaks stand as testaments to human resilience and the enduring power of nature. Their timeless grandeur bears witness to the indomitable spirit of exploration that courses through humanity’s veins, a testament to our unwavering quest for discovery amidst the vast and formidable landscapes of this world.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T20:03:19.157682300Z",
     "start_time": "2023-11-23T20:03:19.095336500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'geo', 'score': 0.985183, 'word': 'sierra nevada', 'start': 279, 'end': 292}, {'entity_group': 'geo', 'score': 0.9863523, 'word': 'swiss alps', 'start': 322, 'end': 332}]\n"
     ]
    }
   ],
   "source": [
    "# Create a Named Entity Recognition (NER) pipeline using the provided model and tokenizer\n",
    "pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Process the 'text' input using the NER pipeline with aggregation strategy set to \"max\"\n",
    "print(pipeline(text, aggregation_strategy=\"max\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T20:03:19.983334300Z",
     "start_time": "2023-11-23T20:03:19.157682300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "# # Use the loaded model for making predictions\n",
    "# model = load_model(\"runs\")\n",
    "# predictions = model.predict(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T20:03:20.030515100Z",
     "start_time": "2023-11-23T20:03:19.983334300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
