{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "from typing import List"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the API\n",
    "openai.api_base = \"http://localhost:1234/v1\"\n",
    "\n",
    "# Create a chat completion request\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"local-model\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a geography expert and provide accurate answers, consulting reliable sources.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Coming from https://www.britannica.com/topic/list-of-mountains-2009175 exclude any additional information, leave only mountain names. Provide answer in JSON format.\"}\n",
    "  ],\n",
    "  temperature=0.2,  # Set the temperature for response generation\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "# Record the start time of the request\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize a list to store received messages\n",
    "collected_messages = list()\n",
    "\n",
    "# Continuously retrieve chunks of messages until completion\n",
    "for chunk in completion:\n",
    "    chunk_time = time.time() - start_time\n",
    "    chunk_message = chunk['choices'][0]['delta']\n",
    "    collected_messages.append(chunk_message)\n",
    "    print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")\n",
    "\n",
    "# Concatenate all collected messages into a full conversation\n",
    "full_reply_content = ''.join([m.get('content', '') for m in collected_messages])\n",
    "print(f\"Full conversation received: {full_reply_content}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Mount Everest',\n",
       "  'location': 'Asia/Nepal/Tibet',\n",
       "  'height': 8848,\n",
       "  'type': 'peak'},\n",
       " {'name': 'K2',\n",
       "  'location': 'Asia/Pakistan/China',\n",
       "  'height': 8611,\n",
       "  'type': 'peak'},\n",
       " {'name': 'Kangchenjunga',\n",
       "  'location': 'Asia/Nepal/India',\n",
       "  'height': 8586,\n",
       "  'type': 'peak'},\n",
       " {'name': 'Lhotse',\n",
       "  'location': 'Asia/Nepal/China',\n",
       "  'height': 8516,\n",
       "  'type': 'peak'},\n",
       " {'name': 'Makalu', 'location': 'Asia/Tibet', 'height': 8481, 'type': 'peak'},\n",
       " {'name': 'Cho Oyu',\n",
       "  'location': 'Asia/Nepal/China',\n",
       "  'height': 8201,\n",
       "  'type': 'peak'},\n",
       " {'name': 'Dhaulagiri',\n",
       "  'location': 'Asia/Nepal',\n",
       "  'height': 8167,\n",
       "  'type': 'peak'},\n",
       " {'name': 'Manaslu', 'location': 'Asia/Nepal', 'height': 8120, 'type': 'peak'},\n",
       " {'name': 'Nanga Parbat',\n",
       "  'location': 'Asia/Pakistan',\n",
       "  'height': 8126,\n",
       "  'type': 'peak'},\n",
       " {'name': 'Annapurna',\n",
       "  'location': 'Asia/Nepal',\n",
       "  'height': 8091,\n",
       "  'type': 'mountain range'},\n",
       " {'name': 'Himalayas',\n",
       "  'location': 'Asia/India/Pakistan',\n",
       "  'height': 8000,\n",
       "  'type': 'mountain range'},\n",
       " {'name': 'Karakoram',\n",
       "  'location': 'Asia/Pakistan/China',\n",
       "  'height': 7934,\n",
       "  'type': 'mountain range'},\n",
       " {'name': 'Tian Shan',\n",
       "  'location': 'Asia/Kazakhstan',\n",
       "  'height': 7900,\n",
       "  'type': 'mountain range'},\n",
       " {'name': 'Hindukush',\n",
       "  'location': 'Afghanistan/Pakistan',\n",
       "  'height': 7841,\n",
       "  'type': 'mountain range'},\n",
       " {'name': 'Kamchatka',\n",
       "  'location': 'Russia',\n",
       "  'height': 7290,\n",
       "  'type': 'volcano'},\n",
       " {'name': 'Mount Kilimanjaro',\n",
       "  'location': 'Africa/Tanzania',\n",
       "  'height': 5895,\n",
       "  'type': 'peak'},\n",
       " {'name': 'Everest Base Camp',\n",
       "  'location': 'Asia/Nepal',\n",
       "  'height': 5360,\n",
       "  'type': 'camp'},\n",
       " {'name': 'K2 Base Camp',\n",
       "  'location': 'Asia/Pakistan/China',\n",
       "  'height': 5189,\n",
       "  'type': 'camp'},\n",
       " {'name': 'Lhotse Base Camp',\n",
       "  'location': 'Asia/Nepal/China',\n",
       "  'height': 5470,\n",
       "  'type': 'camp'},\n",
       " {'name': 'Cho Oyu Base Camp',\n",
       "  'location': 'Asia/Nepal/China',\n",
       "  'height': 5230,\n",
       "  'type': 'camp'},\n",
       " {'name': 'Dhaulagiri Base Camp',\n",
       "  'location': 'Asia/Nepal',\n",
       "  'height': 5189,\n",
       "  'type': 'camp'},\n",
       " {'name': 'Manaslu Base Camp',\n",
       "  'location': 'Asia/Nepal',\n",
       "  'height': 5060,\n",
       "  'type': 'camp'},\n",
       " {'name': 'Nanga Parbat Base Camp',\n",
       "  'location': 'Asia/Pakistan',\n",
       "  'height': 4930,\n",
       "  'type': 'camp'},\n",
       " {'name': 'Annapurna Base Camp',\n",
       "  'location': 'Asia/Nepal',\n",
       "  'height': 5189,\n",
       "  'type': 'camp'},\n",
       " {'name': 'Himalayas Base Camp',\n",
       "  'location': 'Asia/India/Pakistan',\n",
       "  'height': 4700,\n",
       "  'type': 'camp'},\n",
       " {'name': 'Karakoram Base Camp',\n",
       "  'location': 'Asia/Pakistan/China',\n",
       "  'height': 5189,\n",
       "  'type': 'camp'},\n",
       " {'name': 'Tian Shan Base Camp',\n",
       "  'location': 'Asia/Kazakhstan',\n",
       "  'height': 4700,\n",
       "  'type': 'camp'},\n",
       " {'name': 'Hindukush Base Camp',\n",
       "  'location': 'Afghanistan/Pakistan',\n",
       "  'height': 5189,\n",
       "  'type': 'camp'},\n",
       " {'name': 'Kamchatka Base Camp',\n",
       "  'location': 'Russia',\n",
       "  'height': 7290,\n",
       "  'type': 'camp'},\n",
       " {'name': 'Mount Kilimanjaro Base Camp',\n",
       "  'location': 'Africa/Tanzania',\n",
       "  'height': 5360,\n",
       "  'type': 'camp'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming msg is the string containing JSON-like content within square brackets\n",
    "msg = completion.choices[0].message[\"content\"]\n",
    "data = json.loads(msg[msg.find('['):msg.rfind(']') + 1])\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "++Caucasus++ is the highest mountain range in Europe, stretching for over 2,500 km and covering an area of 100,000 square kilometers. It is located between the Black Sea and the Caspian Sea, and its peaks are often shrouded in clouds. The highest peak is Mount Elbrus, which stands at 5,642 meters above sea level.\n",
      "\n",
      "++Caucasus++ is a mountain range that runs along the border of several countries, including Russia"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m/home/nom/python_tests/nb.ipynb Cell 3\u001B[0m line \u001B[0;36m2\n\u001B[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nom/python_tests/nb.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001B[0m start_time \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime()\n\u001B[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nom/python_tests/nb.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001B[0m collected_messages \u001B[39m=\u001B[39m \u001B[39mlist\u001B[39m()\n\u001B[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nom/python_tests/nb.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001B[0m \u001B[39mfor\u001B[39;00m chunk \u001B[39min\u001B[39;00m completion:\n\u001B[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nom/python_tests/nb.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001B[0m     chunk_time \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime() \u001B[39m-\u001B[39m start_time\n\u001B[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/nom/python_tests/nb.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001B[0m     chunk_message \u001B[39m=\u001B[39m chunk[\u001B[39m'\u001B[39m\u001B[39mchoices\u001B[39m\u001B[39m'\u001B[39m][\u001B[39m0\u001B[39m][\u001B[39m'\u001B[39m\u001B[39mdelta\u001B[39m\u001B[39m'\u001B[39m]  \u001B[39m# extract the message\u001B[39;00m\n",
      "File \u001B[0;32m~/arcus/src/arcuscience/advisor_scanner/.venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:166\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    163\u001B[0m \u001B[39mif\u001B[39;00m stream:\n\u001B[1;32m    164\u001B[0m     \u001B[39m# must be an iterator\u001B[39;00m\n\u001B[1;32m    165\u001B[0m     \u001B[39massert\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39misinstance\u001B[39m(response, OpenAIResponse)\n\u001B[0;32m--> 166\u001B[0m     \u001B[39mreturn\u001B[39;00m (\n\u001B[1;32m    167\u001B[0m         util\u001B[39m.\u001B[39mconvert_to_openai_object(\n\u001B[1;32m    168\u001B[0m             line,\n\u001B[1;32m    169\u001B[0m             api_key,\n\u001B[1;32m    170\u001B[0m             api_version,\n\u001B[1;32m    171\u001B[0m             organization,\n\u001B[1;32m    172\u001B[0m             engine\u001B[39m=\u001B[39mengine,\n\u001B[1;32m    173\u001B[0m             plain_old_data\u001B[39m=\u001B[39m\u001B[39mcls\u001B[39m\u001B[39m.\u001B[39mplain_old_data,\n\u001B[1;32m    174\u001B[0m         )\n\u001B[1;32m    175\u001B[0m         \u001B[39mfor\u001B[39;00m line \u001B[39min\u001B[39;00m response\n\u001B[1;32m    176\u001B[0m     )\n\u001B[1;32m    177\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    178\u001B[0m     obj \u001B[39m=\u001B[39m util\u001B[39m.\u001B[39mconvert_to_openai_object(\n\u001B[1;32m    179\u001B[0m         response,\n\u001B[1;32m    180\u001B[0m         api_key,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    184\u001B[0m         plain_old_data\u001B[39m=\u001B[39m\u001B[39mcls\u001B[39m\u001B[39m.\u001B[39mplain_old_data,\n\u001B[1;32m    185\u001B[0m     )\n",
      "File \u001B[0;32m~/arcus/src/arcuscience/advisor_scanner/.venv/lib/python3.10/site-packages/openai/api_requestor.py:692\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    690\u001B[0m \u001B[39m\u001B[39m\u001B[39m\"\"\"Returns the response(s) and a bool indicating whether it is a stream.\"\"\"\u001B[39;00m\n\u001B[1;32m    691\u001B[0m \u001B[39mif\u001B[39;00m stream \u001B[39mand\u001B[39;00m \u001B[39m\"\u001B[39m\u001B[39mtext/event-stream\u001B[39m\u001B[39m\"\u001B[39m \u001B[39min\u001B[39;00m result\u001B[39m.\u001B[39mheaders\u001B[39m.\u001B[39mget(\u001B[39m\"\u001B[39m\u001B[39mContent-Type\u001B[39m\u001B[39m\"\u001B[39m, \u001B[39m\"\u001B[39m\u001B[39m\"\u001B[39m):\n\u001B[0;32m--> 692\u001B[0m     \u001B[39mreturn\u001B[39;00m (\n\u001B[1;32m    693\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_interpret_response_line(\n\u001B[1;32m    694\u001B[0m             line, result\u001B[39m.\u001B[39mstatus_code, result\u001B[39m.\u001B[39mheaders, stream\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m\n\u001B[1;32m    695\u001B[0m         )\n\u001B[1;32m    696\u001B[0m         \u001B[39mfor\u001B[39;00m line \u001B[39min\u001B[39;00m parse_stream(result\u001B[39m.\u001B[39miter_lines())\n\u001B[1;32m    697\u001B[0m     ), \u001B[39mTrue\u001B[39;00m\n\u001B[1;32m    698\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    699\u001B[0m     \u001B[39mreturn\u001B[39;00m (\n\u001B[1;32m    700\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_interpret_response_line(\n\u001B[1;32m    701\u001B[0m             result\u001B[39m.\u001B[39mcontent\u001B[39m.\u001B[39mdecode(\u001B[39m\"\u001B[39m\u001B[39mutf-8\u001B[39m\u001B[39m\"\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    706\u001B[0m         \u001B[39mFalse\u001B[39;00m,\n\u001B[1;32m    707\u001B[0m     )\n",
      "File \u001B[0;32m~/arcus/src/arcuscience/advisor_scanner/.venv/lib/python3.10/site-packages/openai/api_requestor.py:115\u001B[0m, in \u001B[0;36mparse_stream\u001B[0;34m(rbody)\u001B[0m\n\u001B[1;32m    114\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mparse_stream\u001B[39m(rbody: Iterator[\u001B[39mbytes\u001B[39m]) \u001B[39m-\u001B[39m\u001B[39m>\u001B[39m Iterator[\u001B[39mstr\u001B[39m]:\n\u001B[0;32m--> 115\u001B[0m     \u001B[39mfor\u001B[39;00m line \u001B[39min\u001B[39;00m rbody:\n\u001B[1;32m    116\u001B[0m         _line \u001B[39m=\u001B[39m parse_stream_helper(line)\n\u001B[1;32m    117\u001B[0m         \u001B[39mif\u001B[39;00m _line \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/arcus/src/arcuscience/advisor_scanner/.venv/lib/python3.10/site-packages/requests/models.py:865\u001B[0m, in \u001B[0;36mResponse.iter_lines\u001B[0;34m(self, chunk_size, decode_unicode, delimiter)\u001B[0m\n\u001B[1;32m    856\u001B[0m \u001B[39m\u001B[39m\u001B[39m\"\"\"Iterates over the response data, one line at a time.  When\u001B[39;00m\n\u001B[1;32m    857\u001B[0m \u001B[39mstream=True is set on the request, this avoids reading the\u001B[39;00m\n\u001B[1;32m    858\u001B[0m \u001B[39mcontent at once into memory for large responses.\u001B[39;00m\n\u001B[1;32m    859\u001B[0m \n\u001B[1;32m    860\u001B[0m \u001B[39m.. note:: This method is not reentrant safe.\u001B[39;00m\n\u001B[1;32m    861\u001B[0m \u001B[39m\"\"\"\u001B[39;00m\n\u001B[1;32m    863\u001B[0m pending \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m\n\u001B[0;32m--> 865\u001B[0m \u001B[39mfor\u001B[39;00m chunk \u001B[39min\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39miter_content(\n\u001B[1;32m    866\u001B[0m     chunk_size\u001B[39m=\u001B[39mchunk_size, decode_unicode\u001B[39m=\u001B[39mdecode_unicode\n\u001B[1;32m    867\u001B[0m ):\n\u001B[1;32m    869\u001B[0m     \u001B[39mif\u001B[39;00m pending \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[1;32m    870\u001B[0m         chunk \u001B[39m=\u001B[39m pending \u001B[39m+\u001B[39m chunk\n",
      "File \u001B[0;32m~/arcus/src/arcuscience/advisor_scanner/.venv/lib/python3.10/site-packages/requests/models.py:816\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[0;34m()\u001B[0m\n\u001B[1;32m    814\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mhasattr\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mraw, \u001B[39m\"\u001B[39m\u001B[39mstream\u001B[39m\u001B[39m\"\u001B[39m):\n\u001B[1;32m    815\u001B[0m     \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m--> 816\u001B[0m         \u001B[39myield from\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mraw\u001B[39m.\u001B[39mstream(chunk_size, decode_content\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m)\n\u001B[1;32m    817\u001B[0m     \u001B[39mexcept\u001B[39;00m ProtocolError \u001B[39mas\u001B[39;00m e:\n\u001B[1;32m    818\u001B[0m         \u001B[39mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "File \u001B[0;32m~/arcus/src/arcuscience/advisor_scanner/.venv/lib/python3.10/site-packages/urllib3/response.py:933\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[0;34m(self, amt, decode_content)\u001B[0m\n\u001B[1;32m    917\u001B[0m \u001B[39m\u001B[39m\u001B[39m\"\"\"\u001B[39;00m\n\u001B[1;32m    918\u001B[0m \u001B[39mA generator wrapper for the read() method. A call will block until\u001B[39;00m\n\u001B[1;32m    919\u001B[0m \u001B[39m``amt`` bytes have been read from the connection or until the\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    930\u001B[0m \u001B[39m    'content-encoding' header.\u001B[39;00m\n\u001B[1;32m    931\u001B[0m \u001B[39m\"\"\"\u001B[39;00m\n\u001B[1;32m    932\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mchunked \u001B[39mand\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39msupports_chunked_reads():\n\u001B[0;32m--> 933\u001B[0m     \u001B[39myield from\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mread_chunked(amt, decode_content\u001B[39m=\u001B[39mdecode_content)\n\u001B[1;32m    934\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    935\u001B[0m     \u001B[39mwhile\u001B[39;00m \u001B[39mnot\u001B[39;00m is_fp_closed(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_fp) \u001B[39mor\u001B[39;00m \u001B[39mlen\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_decoded_buffer) \u001B[39m>\u001B[39m \u001B[39m0\u001B[39m:\n",
      "File \u001B[0;32m~/arcus/src/arcuscience/advisor_scanner/.venv/lib/python3.10/site-packages/urllib3/response.py:1073\u001B[0m, in \u001B[0;36mHTTPResponse.read_chunked\u001B[0;34m(self, amt, decode_content)\u001B[0m\n\u001B[1;32m   1070\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mNone\u001B[39;00m\n\u001B[1;32m   1072\u001B[0m \u001B[39mwhile\u001B[39;00m \u001B[39mTrue\u001B[39;00m:\n\u001B[0;32m-> 1073\u001B[0m     \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_update_chunk_length()\n\u001B[1;32m   1074\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mchunk_left \u001B[39m==\u001B[39m \u001B[39m0\u001B[39m:\n\u001B[1;32m   1075\u001B[0m         \u001B[39mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/arcus/src/arcuscience/advisor_scanner/.venv/lib/python3.10/site-packages/urllib3/response.py:1001\u001B[0m, in \u001B[0;36mHTTPResponse._update_chunk_length\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    999\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mchunk_left \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[1;32m   1000\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mNone\u001B[39;00m\n\u001B[0;32m-> 1001\u001B[0m line \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_fp\u001B[39m.\u001B[39;49mfp\u001B[39m.\u001B[39;49mreadline()  \u001B[39m# type: ignore[union-attr]\u001B[39;00m\n\u001B[1;32m   1002\u001B[0m line \u001B[39m=\u001B[39m line\u001B[39m.\u001B[39msplit(\u001B[39mb\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m;\u001B[39m\u001B[39m\"\u001B[39m, \u001B[39m1\u001B[39m)[\u001B[39m0\u001B[39m]\n\u001B[1;32m   1003\u001B[0m \u001B[39mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/lib/python3.10/socket.py:705\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[39mwhile\u001B[39;00m \u001B[39mTrue\u001B[39;00m:\n\u001B[1;32m    704\u001B[0m     \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m--> 705\u001B[0m         \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_sock\u001B[39m.\u001B[39;49mrecv_into(b)\n\u001B[1;32m    706\u001B[0m     \u001B[39mexcept\u001B[39;00m timeout:\n\u001B[1;32m    707\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_timeout_occurred \u001B[39m=\u001B[39m \u001B[39mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Define a function to save responses to a file\n",
    "def save_to_file(filename: str, responses: List[str]):\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write('\\n')\n",
    "        f.write('\\n'.join(responses))\n",
    "\n",
    "# Test list of mountains to generate sentences about\n",
    "mountains = ['Caucasus', 'Hindu Kush']\n",
    "# mountains = data\n",
    "\n",
    "# Initialize a list to store generated responses\n",
    "responses = list()\n",
    "\n",
    "# Define batch size for saving responses to a file\n",
    "batch_size = 5\n",
    "\n",
    "# Iterate through each mountain to generate sentences\n",
    "for mountain in mountains:\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"local-model\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a talented writer in many styles and you use it to write every next sentence in a new style. You are also good at detecting mountain names in each sentence and labeling them as a Data Engineer would.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Create at least 10 different sentences about {mountain} that will also mention its name. Apply special formatting: add ++ around mountain names. Write each sentence from a new line.\"}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    # Record start time for response retrieval\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize a list to collect generated messages\n",
    "    collected_messages = list()\n",
    "\n",
    "    # Retrieve messages in chunks until completion\n",
    "    for chunk in completion:\n",
    "        chunk_time = time.time() - start_time\n",
    "        chunk_message = chunk['choices'][0]['delta']\n",
    "        collected_messages.append(chunk_message)\n",
    "        print(chunk_message.get('content', ''), end='')\n",
    "\n",
    "    # Concatenate collected messages into a full reply content\n",
    "    full_reply_content = ''.join([m.get('content', '') for m in collected_messages])\n",
    "    print(f\"Full conversation received: {full_reply_content}\")\n",
    "\n",
    "    # Append the full reply content to the responses list\n",
    "    responses.append(full_reply_content)\n",
    "\n",
    "    # Check if responses batch size has been reached\n",
    "    if len(responses) > batch_size:\n",
    "        save_to_file('gora_vysokaya.txt', responses)\n",
    "        responses = list()\n",
    "\n",
    "# Save data to file\n",
    "save_to_file('merged_new.txt', responses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
